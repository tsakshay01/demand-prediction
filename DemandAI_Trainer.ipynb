{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ‘— DemandAI - H&M Fashion Training Notebook (Enterprise + Transformer + Max Training)\n",
                "This notebook trains your AI on **H&M Personalized Fashion Data**.\n",
                "\n",
                "### **Instructions**\n",
                "1.  **Menu**: Runtime > Change runtime type > Select **T4 GPU** (Faster training).\n",
                "2.  **Run Step 1**: Install Dependencies.\n",
                "3.  **Run Step 2**: Generate High-Fidelity H&M Synthetic Data (200,000 Rows).\n",
                "4.  **Run Step 3**: Train the **Transformer** Deep Learning Model (50 Epochs).\n",
                "5.  **Run Step 4**: Download your new `model.h5`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# [STEP 1] Install AI Libraries\n",
                "!pip install tensorflow pandas scikit-learn numpy matplotlib faker\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import tensorflow as tf\n",
                "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
                "import matplotlib.pyplot as plt\n",
                "from faker import Faker\n",
                "import random\n",
                "\n",
                "print(\"âœ… Libraries Installed Successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# [STEP 2] Generate H&M Style Dataset\n",
                "# Enterprise Scale: 200,000 Rows to simulate 'Real Data' load.\n",
                "\n",
                "print(\"ðŸ§µ Generating H&M Fashion Dataset (200k Transactions)...\")\n",
                "fake = Faker()\n",
                "data = []\n",
                "categories = ['Ladieswear', 'Divided', 'Menswear', 'Baby/Children', 'Sport']\n",
                "product_types = ['Trousers', 'Dress', 'Sweater', 'T-shirt', 'Jacket']\n",
                "\n",
                "for _ in range(200000):  # 200,000 Transactions\n",
                "    date = fake.date_between(start_date='-2y', end_date='today') # Increased history to 2 years\n",
                "    cat = random.choice(categories)\n",
                "    ptype = random.choice(product_types)\n",
                "    price = round(random.uniform(10.99, 99.99), 2)\n",
                "    \n",
                "    # Demand Logic: Winter sells more Sweaters, Summer sells T-shirts\n",
                "    base_vol = random.randint(10, 50)\n",
                "    season_mult = 1.0\n",
                "    month = date.month\n",
                "    \n",
                "    if ptype == 'Sweater' and month in [11, 12, 1, 2]: season_mult = 2.5\n",
                "    if ptype == 'T-shirt' and month in [6, 7, 8]: season_mult = 2.0\n",
                "    \n",
                "    sales = int(base_vol * season_mult)\n",
                "    \n",
                "    data.append([date, cat, ptype, price, sales])\n",
                "\n",
                "df = pd.DataFrame(data, columns=['t_dat', 'index_group_name', 'product_type_name', 'price', 'sales_channel_id'])\n",
                "df['sales'] = df['sales_channel_id'] # remap for demo\n",
                "df = df.sort_values('t_dat')\n",
                "print(f\"âœ… Generated {len(df)} H&M Transactions.\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# [STEP 3] Train Valid Transformer Model\n",
                "\n",
                "# 1. Preprocess\n",
                "scaler = MinMaxScaler()\n",
                "data_seq = df['sales'].values.reshape(-1, 1)\n",
                "scaled_data = scaler.fit_transform(data_seq)\n",
                "\n",
                "# 2. Create Sequences\n",
                "X_train, y_train = [], []\n",
                "lookback = 30\n",
                "\n",
                "for i in range(lookback, len(scaled_data)):\n",
                "    X_train.append(scaled_data[i-lookback:i, 0])\n",
                "    y_train.append(scaled_data[i, 0])\n",
                "\n",
                "X_train, y_train = np.array(X_train), np.array(y_train)\n",
                "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
                "\n",
                "# 3. Build TRANSFORMER Model (Attention Mechanism)\n",
                "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
                "    # Attention and Normalization\n",
                "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
                "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
                "    x = Dropout(dropout)(x)\n",
                "    res = x + inputs\n",
                "\n",
                "    # Feed Forward Part\n",
                "    x = LayerNormalization(epsilon=1e-6)(res)\n",
                "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
                "    x = Dropout(dropout)(x)\n",
                "    x = Dense(inputs.shape[-1])(x)\n",
                "    return x + res\n",
                "\n",
                "inputs = Input(shape=(lookback, 1))\n",
                "x = transformer_encoder(inputs, head_size=256, num_heads=4, ff_dim=4, dropout=0.25)\n",
                "x = GlobalAveragePooling1D()(x)\n",
                "x = Dropout(0.1)(x)\n",
                "outputs = Dense(1)(x)\n",
                "\n",
                "model = Model(inputs, outputs)\n",
                "model.compile(optimizer='adam', loss='mean_squared_error')\n",
                "\n",
                "# 4. Train\n",
                "print(\"ðŸš€ Training TRANSFORMER Brain on H&M Data (50 Epochs)...\")\n",
                "history = model.fit(X_train, y_train, batch_size=64, epochs=50, verbose=1)\n",
                "print(\"âœ… Training Complete (Transformer Architecture)!\")\n",
                "\n",
                "# Plot Loss\n",
                "plt.plot(history.history['loss'])\n",
                "plt.title('Transformer Training Loss')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# [STEP 4] Download Trained Brain\n",
                "model.save('model.h5')\n",
                "from google.colab import files\n",
                "files.download('model.h5')\n",
                "print(\"âœ… 'model.h5' downloaded! Upload this to your Dashboard.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}